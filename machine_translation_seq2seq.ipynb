{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation: seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9953843907868990253\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 357302272\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 14935192867186758903\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "Due to limited computing power for AWS free tier EC2 instance that I am using, dataset used for this task contains small vocabulary (200 ~ 300 words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "with open('data/small_vocab_en', 'r') as f:\n",
    "    eng_sentences = f.read().split('\\n')\n",
    "    \n",
    "with open('data/small_vocab_fr', 'r') as f:\n",
    "    fre_sentences = f.read().split('\\n')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence 1 :  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "French Sentence 1  :  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "English Sentence 2 :  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "French Sentence 2  :  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('English Sentence {} :  {}'.format(sample_i+1, eng_sentences[sample_i]))\n",
    "    print('French Sentence {}  :  {}\\n'.format(sample_i+1, fre_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-process text\n",
    "## 2.1. Tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(x, encode_start_end = False):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :param encode_start_end: if True, pad the start & end of sentence as separate tokens\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    if encode_start_end:\n",
    "        x = [\"startofsentence \" + sentence + \" endofsentence\" for sentence in x]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    tokenized_x = tokenizer.texts_to_sequences(x)\n",
    "    \n",
    "    return tokenized_x, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Padding  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "        \n",
    "    padded_x = pad_sequences(x, maxlen = length, padding = 'post', truncating = 'post')\n",
    "    \n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Execute both functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size:  199\n",
      "frenish vocabulary size:  346\n",
      "\n",
      "Length of longest English sentence:  15\n",
      "Length of longest frenish sentence:  23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eng_tokenized, eng_tokenizer = tokenize(eng_sentences)\n",
    "fre_tokenized, fre_tokenizer = tokenize(fre_sentences, encode_start_end = True)\n",
    "\n",
    "eng_encoded = pad(eng_tokenized)\n",
    "fre_encoded = pad(fre_tokenized)\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index)\n",
    "fre_vocab_size = len(fre_tokenizer.word_index)\n",
    "\n",
    "print(\"English vocabulary size: \", eng_vocab_size)\n",
    "print(\"frenish vocabulary size: \", fre_vocab_size)\n",
    "print()\n",
    "  \n",
    "eng_seq_len = len(eng_encoded[0])\n",
    "fre_seq_len = len(fre_encoded[0])\n",
    "        \n",
    "print(\"Length of longest English sentence: \", eng_seq_len)\n",
    "print(\"Length of longest frenish sentence: \", fre_seq_len)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Seq2Seq Model & Train\n",
    "## 3.1. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/16\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 1.8692 - acc: 0.5911 - val_loss: nan - val_acc: 0.7214\n",
      "Epoch 2/16\n",
      "110288/110288 [==============================] - 21s 195us/step - loss: 0.7414 - acc: 0.7552 - val_loss: nan - val_acc: 0.7793\n",
      "Epoch 3/16\n",
      "110288/110288 [==============================] - 22s 196us/step - loss: 0.5679 - acc: 0.8039 - val_loss: nan - val_acc: 0.8177\n",
      "Epoch 4/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.4673 - acc: 0.8403 - val_loss: nan - val_acc: 0.8550\n",
      "Epoch 5/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.3995 - acc: 0.8640 - val_loss: nan - val_acc: 0.8785\n",
      "Epoch 6/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.3279 - acc: 0.8896 - val_loss: nan - val_acc: 0.8968\n",
      "Epoch 7/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.2478 - acc: 0.9169 - val_loss: nan - val_acc: 0.9330\n",
      "Epoch 8/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.1571 - acc: 0.9488 - val_loss: nan - val_acc: 0.9677\n",
      "Epoch 9/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.1047 - acc: 0.9696 - val_loss: nan - val_acc: 0.9805\n",
      "Epoch 10/16\n",
      "110288/110288 [==============================] - 22s 196us/step - loss: 0.0588 - acc: 0.9842 - val_loss: nan - val_acc: 0.9847\n",
      "Epoch 11/16\n",
      "110288/110288 [==============================] - 21s 194us/step - loss: 0.0482 - acc: 0.9873 - val_loss: nan - val_acc: 0.9870\n",
      "Epoch 12/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.0663 - acc: 0.9825 - val_loss: nan - val_acc: 0.9888\n",
      "Epoch 13/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.0374 - acc: 0.9908 - val_loss: nan - val_acc: 0.9901\n",
      "Epoch 14/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.0409 - acc: 0.9898 - val_loss: nan - val_acc: 0.9895\n",
      "Epoch 15/16\n",
      "110288/110288 [==============================] - 22s 195us/step - loss: 0.0330 - acc: 0.9923 - val_loss: nan - val_acc: 0.9913\n",
      "Epoch 16/16\n",
      "110288/110288 [==============================] - 21s 195us/step - loss: 0.0291 - acc: 0.9935 - val_loss: nan - val_acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feddf9ac438>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "english_input = ed_preproc_english_sentences   # (137861, 15,)\n",
    "\n",
    "decoder_french_input = ed_preproc_french_sentences[:, :-1, :]\n",
    "decoder_french_target = ed_preproc_french_sentences[:, 1:, :]\n",
    "    \n",
    "# 1. Define Encoder\n",
    "input_seq_encoder = Input(shape = (None, ),\n",
    "                          name = \"encoder_input\")     # (batch_size, sentence_length, 1) \n",
    "\n",
    "embed_dim = 200\n",
    "embedded_seq_encoder = Embedding(input_dim = english_vocab_size, \n",
    "                                 output_dim = embed_dim)(input_seq_encoder)\n",
    "\n",
    "encoder_lstm = LSTM(units = 256,             \n",
    "                    activation = 'relu',\n",
    "                    return_sequences = False,\n",
    "                    return_state = True,\n",
    "                    name = \"encoder_LSTM\")\n",
    "\n",
    "_, last_hidden_encoder, last_cell_encoder = encoder_lstm(embedded_seq_encoder)\n",
    "\n",
    "\n",
    "# 2. Define Decoder\n",
    "input_seq_decoder = Input(shape = (None, 1),\n",
    "                          name = \"decoder_input\")     # (batch_size, sentence_length, 1)\n",
    "\n",
    "decoder_lstm = LSTM(units = 256,                          \n",
    "                    activation = 'relu',\n",
    "                    return_sequences = True,\n",
    "                    return_state = True,\n",
    "                    name = \"decoder_LSTM\")\n",
    "\n",
    "all_hidden_decoder, _, _ = decoder_lstm(input_seq_decoder, \n",
    "                                        initial_state = [last_hidden_encoder, last_cell_encoder])\n",
    "\n",
    "decoder_dense = Dense(ed_french_vocab_size,   # NOT TIMEDISTRIBUTED (NOT RECURSIVE)\n",
    "                      activation = 'softmax',\n",
    "                      name = \"decoder_dense\")\n",
    "logits = decoder_dense(all_hidden_decoder)\n",
    "\n",
    "\n",
    "# 3. Define Model\n",
    "final_rnn_model = Model(input = [input_seq_encoder, input_seq_decoder],\n",
    "                        output = logits)\n",
    "\n",
    "final_rnn_model.compile(loss = sparse_categorical_crossentropy,\n",
    "                        optimizer = Adam(lr = 0.002),\n",
    "                        metrics = ['accuracy'])\n",
    "\n",
    "# 4. Fit the Model\n",
    "final_rnn_model.fit([english_input, decoder_french_input],\n",
    "                    decoder_french_target,\n",
    "                    batch_size = 1024,\n",
    "                    epochs = 16,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    39800       encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_LSTM (LSTM)             [(None, 256), (None, 467968      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_LSTM (LSTM)             [(None, None, 256),  264192      decoder_input[0][0]              \n",
      "                                                                 encoder_LSTM[0][1]               \n",
      "                                                                 encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 346)    88922       decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 860,882\n",
      "Trainable params: 860,882\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Inference model\n",
    "### 2.1. Encoder Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"en..., outputs=[<tf.Tenso...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "last_states_encoder = [last_hidden_encoder, last_cell_encoder]\n",
    "inference_encoder_model = Model(input = input_seq_encoder, \n",
    "                                output = last_states_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Decoder Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "decoder_initial_state = [Input(shape = (256,)), Input(shape = (256,))]  \n",
    "all_hidden_decoder, last_hidden_decoder, last_cell_decoder = decoder_lstm(input_seq_decoder, \n",
    "                                                                          initial_state = decoder_initial_state)\n",
    "\n",
    "logits = decoder_dense(all_hidden_decoder)\n",
    "\n",
    "inference_decoder_model = Model(input  = [input_seq_decoder] + decoder_initial_state, \n",
    "\n",
    "                                output = [logits, \n",
    "                                          last_hidden_decoder, \n",
    "                                          last_cell_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Decode Sequence Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_to_word = {idx:word for word, idx in ed_french_tokenizer.word_index.items()}\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model defined above\n",
    "    :param input_seq: (list) encoded english sentence (list of word ids)\n",
    "    returns : translated French sentence\n",
    "    \"\"\"\n",
    "    decoder_input = inference_encoder_model.predict(input_seq)\n",
    "\n",
    "    # Initialize decoder input as a length 1 sentence containing \"startofsentence\",\n",
    "    # --> feeding the start token as the first predicted word\n",
    "    prev_word = np.zeros((1, 1, 1))\n",
    "    prev_word[0, 0, 0] = ed_french_tokenizer.word_index[\"startofsentence\"]\n",
    "\n",
    "    stop_condition = False\n",
    "    translation = []\n",
    "    while not stop_condition:\n",
    "        # 1. predict the next word using decoder model\n",
    "        logits, last_h, last_c = inference_decoder_model.predict([prev_word] + decoder_input)\n",
    "        \n",
    "        # 2. Update prev_word with the predicted word\n",
    "        predicted_id = np.argmax(logits[0, 0, :])\n",
    "        predicted_word = target_id_to_word[predicted_id]\n",
    "        decoded_sentence.append(predicted_word)\n",
    "\n",
    "        # 3. Enable End Condition: (1) if predicted word is \"endofsentence\" OR\n",
    "        #                          (2) if translated sentence reached maximum sentence length\n",
    "        if (predicted_word == 'endofsentence' or len(translation) > decoder_french_target.shape[1]):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 4. Update prev_word with the predicted word\n",
    "        prev_word[0, 0, 0] = predicted_id\n",
    "\n",
    "        # 5. Update initial_states with the previously predicted word's encoder output\n",
    "        decoder_input = [last_h, last_c]\n",
    "\n",
    "    return \" \".join(decoded_sentence).replace('endofsentence', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence            :  i plan to visit france in spring .\n",
      "Predicted French Translation:  je prévois de visiter la france au printemps \n",
      "Correct French Translation  :  je prévois de visiter la france au printemps .\n",
      "\n",
      "English Sentence            :  she likes grapes , apples , and grapefruit.\n",
      "Predicted French Translation:  elle aime les raisins les pommes et le pamplemousse \n",
      "Correct French Translation  :  elle aime les raisins , les pommes et le pamplemousse .\n",
      "\n",
      "English Sentence            :  my most loved animal was that bird .\n",
      "Predicted French Translation:  mon animal le plus aimé était cet oiseau \n",
      "Correct French Translation  :  mon animal le plus aimé était cet oiseau .\n",
      "\n",
      "English Sentence            :  france is pleasant during july , but it is usually dry in december .\n",
      "Predicted French Translation:  la france est agréable en juillet mais il est généralement sec en décembre \n",
      "Correct French Translation  :  la france est agréable en juillet , mais il est généralement sec en décembre .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print prediction(s)\n",
    "for i in [293, 296, 393, 418]:\n",
    "    english_seq = ed_preproc_english_sentences[i].reshape(1, ed_preproc_english_sentences.shape[1])\n",
    "    french_translation = decode_sequence(english_seq)\n",
    "    \n",
    "    print(\"English Sentence            : \", english_sentences[i])\n",
    "    print(\"Predicted French Translation: \", french_translation)\n",
    "    print(\"Correct French Translation  : \", french_sentences[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like perfect translations for the standarad of the simple dataset with limited vocabulary!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
